# ğŸ—‚ï¸ Code Structure

This page provides a high-level overview of the main components in **ViT-SSL** and how they fit together.

## `vit_core`
Core implementation of the Vision Transformer (ViT) and self-supervised variants.

- `patch_embedding.py` â€“ splits the image into patches and projects them to the embedding dimension.
- `attention.py` â€“ multi-head self-attention layer used inside each transformer block.
- `feed_forward.py` â€“ MLP block that follows the attention mechanism.
- `encoder_block.py` â€“ combines attention and feed-forward layers into a single transformer block.
- `mlp_head.py` â€“ classification head applied to the final representation.
- `vit.py` â€“ convenience wrapper assembling the full ViT model.
- `ssl/` â€“ implementations of DINO and SimMIM specific components.

## `utils`
Utilities used throughout training and evaluation.

- `trainers/` â€“ training loops for Supervised, DINO and SimMIM methods.
- `logger.py` â€“ colored logging based on the `rich` library.
- `metrics.py` â€“ common evaluation metrics.
- `schedulers.py` â€“ helper learning rate schedulers.

## `configs`
[Hydra](https://hydra.cc/) configuration files. Base configs live under `configs/base/` and each method has overrides (e.g. `configs/dino/`, `configs/simmim/`). The root `config.yaml` selects which set of overrides to apply.

For details on each configurability, see the [Configuration Guide](configs.md) section.

## Other directories

- `data/` â€“ small dataset wrappers used for experiments.
- `scripts/` â€“ evaluation and helper scripts.
- `tests/` â€“ PyTest unit tests for the core modules.
- `train.py` â€“ main entry point that loads a config and launches training.

Use these modules together to experiment with different SSL approaches and fine-tuning scenarios.

For details on each algorithm, see the [Training Methods](training/index.md) section.