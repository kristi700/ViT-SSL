{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\udc7e Welcome to ViT-SSL","text":"<p>A PyTorch framework for Self-Supervised and Supervised Learning with Vision Transformers.</p> <p>ViT-SSL is an educational project designed to demonstrate the core ideas of Self-Supervised Learning (SSL) through modular, readable, and reproducible code. It\u2019s currently under development and aims to make SSL concepts more accessible to developers and researchers.</p> <p>This site contains detailed documentation of the project\u2019s structure \u2014 explaining what each module does, how components interact, and why certain design decisions were made.</p>"},{"location":"#overview","title":"\u2728 Overview","text":"<p>ViT-SSL provides:  - Implementations of modern SSL methods: DINO, SimMIM, and Supervised baselines - Customizable training pipelines built with PyTorch - Clear config-driven design for experimentation - Metrics and logging tailored for representation learning</p>"},{"location":"#installation","title":"\u2699\ufe0f Installation","text":"<ol> <li> <p>Clone the repository: <code>bash     git clone https://github.com/kristi700/ViT-SSL.git     cd ViT-SSL</code></p> </li> <li> <p>Create a virtual environment (recommended): <code>bash     python -m venv venv     source venv/bin/activate</code></p> </li> <li> <p>Install dependencies: <code>bash     pip install -r requirements.txt</code></p> </li> </ol>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick Start","text":"<p>Training is unified under a single entry point: <code>train.py</code>. To switch between methods (e.g., Supervised, DINO, SimMIM, Fine-tuning), simply set the <code>training.type</code> field in the config file.</p> <p>\ud83d\udee0\ufe0f For more on available config options, see Configuration Guide.</p> <p>\ud83d\uddc2\ufe0f For a tour of the codebase, see Code Structure.</p>"},{"location":"architecture/","title":"\ud83d\uddc2\ufe0f Code Structure","text":"<p>This page provides a high-level overview of the main components in ViT-SSL and how they fit together.</p>"},{"location":"architecture/#vit_core","title":"<code>vit_core</code>","text":"<p>Core implementation of the Vision Transformer (ViT) and self-supervised variants.</p> <ul> <li><code>patch_embedding.py</code> \u2013 splits the image into patches and projects them to the embedding dimension.</li> <li><code>attention.py</code> \u2013 multi-head self-attention layer used inside each transformer block.</li> <li><code>feed_forward.py</code> \u2013 MLP block that follows the attention mechanism.</li> <li><code>encoder_block.py</code> \u2013 combines attention and feed-forward layers into a single transformer block.</li> <li><code>mlp_head.py</code> \u2013 classification head applied to the final representation.</li> <li><code>vit.py</code> \u2013 convenience wrapper assembling the full ViT model.</li> <li><code>ssl/</code> \u2013 implementations of DINO and SimMIM specific components.</li> </ul>"},{"location":"architecture/#utils","title":"<code>utils</code>","text":"<p>Utilities used throughout training and evaluation.</p> <ul> <li><code>scripts/</code> \u2013 Visualization scripts for supervisely trained models and SimMIM. </li> <li><code>trainers/</code> \u2013 training loops for Supervised, DINO and SimMIM methods.</li> <li><code>logger.py</code> \u2013 colored logging based on the <code>rich</code> library.</li> <li><code>metrics.py</code> \u2013 common evaluation metrics.</li> <li><code>schedulers.py</code> \u2013 helper learning rate schedulers.</li> </ul>"},{"location":"architecture/#configs","title":"<code>configs</code>","text":"<p>Hydra configuration files. Base configs live under <code>configs/base/</code> and each method has overrides (e.g. <code>configs/dino/</code>, <code>configs/simmim/</code>). The root <code>config.yaml</code> selects which set of overrides to apply.</p> <p>For details on each configurability, see the Configuration Guide section.</p>"},{"location":"architecture/#other-directories","title":"Other directories","text":"<ul> <li><code>data/</code> \u2013 small dataset wrappers used for experiments (data format).</li> <li><code>evaluators/</code> \u2013 evaluation and helper scripts. See Evaluation Scripts for usage examples.</li> <li><code>tests/</code> \u2013 PyTest unit tests for the core modules.</li> <li><code>train.py</code> \u2013 main entry point that loads a config and launches training.</li> </ul> <p>Use these modules together to experiment with different SSL approaches and fine-tuning scenarios.</p> <p>For details on each algorithm, see the Training Methods section.</p>"},{"location":"configs/","title":"\ud83d\udee0\ufe0f Configuration Guide","text":""},{"location":"configs/#configuration-structure","title":"Configuration Structure","text":"<p>The project uses Hydra for modular, hierarchical configuration management. Configs are split into base definitions and method-specific overrides (DINO, SimMIM, Supervised, Fine-tuning).</p>"},{"location":"configs/#directory-structure","title":"Directory Structure","text":"<pre><code> \nconfigs/\n\u251c\u2500\u2500 base/ # Shared defaults for all training types \n\u2502 \u251c\u2500\u2500 data.yaml \n\u2502 \u251c\u2500\u2500 model.yaml \n\u2502 \u2514\u2500\u2500 training.yaml \n\u251c\u2500\u2500 dino/ # DINO-specific overrides \n\u2502 \u251c\u2500\u2500 data.yaml \n\u2502 \u251c\u2500\u2500 model.yaml \n\u2502 \u251c\u2500\u2500 training.yaml \n\u2502 \u251c\u2500\u2500 globals.yaml \n\u2502 \u251c\u2500\u2500 locals.yaml \n\u2502 \u2514\u2500\u2500 metrics.yaml \n\u251c\u2500\u2500 simmim/ # SimMIM-specific overrides \n\u251c\u2500\u2500 supervised/ # Supervised-specific overrides \n\u251c\u2500\u2500 finetune/ # Fine-tuning-specific overrides \n\u251c\u2500\u2500 dino.yaml # Hydra composition entrypoint for DINO \n\u251c\u2500\u2500 simmim.yaml \n\u251c\u2500\u2500 supervised.yaml \n\u251c\u2500\u2500 finetune.yaml \n\u2514\u2500\u2500 config.yaml # Root selector that loads a full training config </code></pre>"},{"location":"configs/#base-shared-configs","title":"<code>base/</code>: Shared Configs","text":"<p>Used by all training types to define common logic:</p> <ul> <li><code>data.yaml</code>: dataset name, directory, val split, image size, etc.</li> <li><code>model.yaml</code>: patch size, embedding dim, block/head count, etc.</li> <li><code>training.yaml</code>: generic hyperparameters, LR schedulers, optimizers</li> </ul>"},{"location":"configs/#method-specific-overrides","title":"Method-Specific Overrides","text":"<p>Each training strategy has its own folder (e.g. <code>dino/</code>, <code>simmim/</code>) that overrides or extends the base values.</p> <p>Example from <code>dino.yaml</code>:</p> <pre><code>defaults:\n  - config_schema\n\n  - base@data: data\n  - base@model: model\n  - base@training: training\n\n  - dino@data: data\n  - dino@model: model\n  - dino@training: training\n  - dino@transforms.globals: globals\n  - dino@transforms.locals: locals\n  - dino@metrics: metrics\n\nhydra:\n  run:\n    dir: ./experiments/${training.type}/${now:%Y-%m-%d_%H_%M_%S}\n</code></pre> <p>Root Composition (config.yaml)</p> <p>Sets the active method (e.g. DINO, SimMIM):</p> <pre><code>defaults:\n  - dino.yaml\n  - _self_\n</code></pre> <p>To switch training modes, change dino.yaml to simmim.yaml, finetune.yaml, etc.</p> <p>Output Directory</p> <p>Each run saves outputs in a method+timestamp-based directory:</p> <p><code>./experiments/training.type/timestamp/</code></p> <p>This structure keeps logs, configs, and checkpoints isolated per run.</p>"},{"location":"data/","title":"\ud83d\udcca Data Format","text":"<p>This page describes the lightweight dataset wrappers under <code>data/datasets.py</code> and how input files are organized.</p>"},{"location":"data/#cifar10dataset","title":"<code>CIFAR10Dataset</code>","text":"<ul> <li>CSV annotations \u2013 two columns: image filename without extension and label.</li> <li>Image folder \u2013 <code>root_dir</code> contains the corresponding <code>*.png</code> files.</li> </ul> <pre><code>images/\n  00001.png\n  00002.png\n...\ntrain.csv\n</code></pre> <p>Each row in <code>train.csv</code> contains <code>image_name,label</code>. Labels are mapped to indices using the unique values discovered in the file.</p>"},{"location":"data/#stl10dataset","title":"<code>STL10Dataset</code>","text":"<ul> <li>JSON annotations \u2013 similar two-field structure <code>[\"file\",\"label\"]</code>.</li> <li>Image folder \u2013 <code>root_dir</code> points to the directory with <code>.png</code> images.</li> </ul> <p>The JSON file can store absolute paths or paths relative to the image folder. Only the final filename is used to load the image.</p>"},{"location":"data/#stl10unsuperviseddataset","title":"<code>STL10UnsupervisedDataset</code>","text":"<p>Used for unlabeled data. Simply provide a directory with images. Filenames are sorted alphabetically to create a deterministic order.</p>"},{"location":"data/#stl10dinodataset","title":"<code>STL10DINODataset</code>","text":"<p>Special dataset for DINO pretraining. It loads images from a folder like <code>STL10UnsupervisedDataset</code> but additionally generates multiple global and local views using provided transform functions. Parameters:</p> <ul> <li><code>num_all_views</code> \u2013 total number of views to create.</li> <li><code>num_global_views</code> \u2013 how many of those views should cover more than half of the image area.</li> </ul>"},{"location":"data/#directory-expectations","title":"Directory expectations","text":"<ul> <li>Images should be stored as <code>.png</code> files.</li> <li>Annotation files (<code>.csv</code> or <code>.json</code>) reside alongside or above the image folder.</li> <li><code>root_dir</code> passed to the dataset points to the folder containing the images.</li> </ul> <p>These simple structures keep the examples short and focused on the SSL algorithms rather than data loading.</p>"},{"location":"evaluation/","title":"\ud83d\udd0d Evaluation Scripts","text":"<p>This section describes the small helper scripts used to analyse trained checkpoints.</p> <ul> <li>Supervised Evaluator</li> <li>Unsupervised Evaluator</li> <li>k\u2011NN Classification</li> <li>Linear Probing</li> <li>UMAP Visualization</li> </ul>"},{"location":"evaluation/knn_classification/","title":"\ud83d\udcca <code>knn_classification.py</code>","text":"<p>Performs a simple k\u2011NN classification on features extracted from a self-supervised model.</p> <p>The number of neighbours defaults to the dataset's class count. The script prints the Top\u20111 accuracy and stores predictions in <code>evaluation_summary.csv</code> when run through <code>unsupervised_evaluator.py</code>.</p>"},{"location":"evaluation/linear_probing/","title":"\ud83d\udcdd <code>linear_probing.py</code>","text":"<p>Trains a logistic regression classifier on the extracted features to measure linear separability.</p> <p>It outputs the Top\u20111 accuracy and is typically launched via <code>unsupervised_evaluator.py</code> alongside other modes.</p>"},{"location":"evaluation/supervised_evaluator/","title":"\ud83d\udccf <code>supervised_dataset_evaluator.py</code>","text":"<p>Evaluates a trained supervised ViT on an entire dataset. The script loads the configuration stored inside the checkpoint, builds the dataloader and reports classification metrics.</p>"},{"location":"evaluation/supervised_evaluator/#usage","title":"Usage","text":"<pre><code>python evaluators/supervised_dataset_evaluator.py checkpoint=path/to/model.pth\n</code></pre> <p>Predictions and the computed Top\u20111 accuracy are written to <code>evaluation_results.json</code> and <code>predictions.csv</code> in the current directory. Pass <code>eval.confusion_matrix=true</code> in the Hydra command to additionally store a confusion matrix.</p>"},{"location":"evaluation/umap_visualization/","title":"\ud83d\uddfa\ufe0f <code>umap_visualization.py</code>","text":"<p>Reduces high\u2011dimensional features to two dimensions with UMAP and creates several plots to inspect the learned representations. It also computes clustering metrics such as silhouette score and adjusted Rand index.</p> <p>The resulting images and quality reports are saved next to the evaluated experiment.</p>"},{"location":"evaluation/unsupervised_evaluator/","title":"\ud83d\udd2c <code>unsupervised_evaluator.py</code>","text":"<p>Runs feature-based evaluations for self-supervised checkpoints. It can execute multiple modes in one pass:</p> <ul> <li><code>eval_knn</code> \u2013 k\u2011NN classification on extracted features.</li> <li><code>eval_linear</code> \u2013 logistic regression (linear probing).</li> <li><code>eval_umap</code> \u2013 dimensionality reduction and clustering metrics with UMAP.</li> </ul> <p>Call it directly to analyse a saved experiment:</p> <pre><code>python evaluators/unsupervised_evaluator.py eval.mode='eval_knn,eval_linear,eval_umap' eval.experiment_path=path/to/exp\n</code></pre> <p>The script loads the model, extracts features once, then applies the selected evaluations. Results are saved to the experiment folder.</p>"},{"location":"evaluation/unsupervised_evaluator/#automatic-integration","title":"Automatic integration","text":"<p>During DINO training, if <code>eval.interval</code> is set in the config, the trainer will automatically call <code>run_evaluation</code> every N epochs. Each evaluation run is stored under the training output directory with the epoch number.</p>"},{"location":"training/","title":"\ud83d\udcda Training Methods","text":"<p>This section explains the learning strategies implemented in ViT-SSL.</p> <ul> <li>DINO \u2013 a self-distillation approach for learning without labels.</li> <li>SimMIM \u2013 masked image modeling that reconstructs hidden patches.</li> <li>Supervised \u2013 standard classification.</li> <li>Fine-tuning \u2013 adapt a pretrained model to a new task.</li> </ul>"},{"location":"training/dino/","title":"\ud83e\udd96 DINO","text":"<p>The DINO implementation in ViT-SSL is modular and educational by design. It follows the original DINO paper.</p>"},{"location":"training/dino/#overview","title":"Overview","text":"<p>DINO (Self-Distillation with No Labels) uses a teacher\u2013student framework to learn representations without supervision. The teacher is an EMA (exponential moving average) of the student. Training aims to align their output distributions using softmax cross-view consistency.</p>"},{"location":"training/dino/#architecture","title":"Architecture","text":"<ul> <li>Defined in <code>vit_core/ssl/dino/model.py</code></li> <li>Composed of:<ul> <li><code>ViTBackbone</code>: shared ViT encoder used for both student and teacher</li> <li><code>DINOHead</code>: MLP head with optional normalization and projection</li> </ul> </li> <li>Initializes:<ul> <li><code>student_backbone</code> \u2190 trainable</li> <li><code>teacher_backbone</code> \u2190 frozen copy, updated with momentum</li> <li>Separate heads for student and teacher</li> <li>Center buffer for output normalization (Eq. 4 in the paper)</li> </ul> </li> </ul> <pre><code>teacher_output = teacher_head(teacher_backbone(x))\nstudent_output = student_head(student_backbone(x))\n</code></pre> <p>Teacher outputs are updated via momentum_update_teacher() using a scheduled momentum.</p>"},{"location":"training/dino/#forward-pass","title":"Forward Pass","text":"<pre><code>def forward(multi_crop_views, num_global_views):\n    student_input = torch.cat(all_views)\n    teacher_input = torch.cat(global_views)\n\n    student_output = student(student_input)\n    teacher_output = teacher(teacher_input)\n\n    return teacher_output, student_output\n</code></pre>"},{"location":"training/dino/#loss-dinoloss","title":"Loss: DINOLoss","text":"<ul> <li>Implements Equation 1 from the DINO paper</li> <li>Applies temperature scaling + centering to the teacher logits</li> <li>Uses cross-view prediction: student tries to predict teacher output from different views</li> <li>Cross entropy is computed between softmaxed teacher and log-softmaxed student outputs:</li> </ul> <p><code>loss = -(softmax(teacher) * log_softmax(student)).sum().mean()</code> </p> <p>Defined in vit_core/ssl/dino/loss.py</p>"},{"location":"training/dino/#training-dinotrainer","title":"Training: DINOTrainer","text":"<ul> <li>Inherits from a generic BaseTrainer</li> <li> <p>Implements:</p> <ul> <li>create_criterion(): builds the DINOLoss</li> <li>train_epoch(): training logic, view reshaping, loss calc, teacher update, warmup</li> <li>validate(): similar logic without gradient computation</li> </ul> </li> <li> <p>Highlights</p> <ul> <li>Teacher momentum is scheduled with a cosine scheduler via DINOMomentumScheduler</li> <li>Both teacher and student outputs are reshaped per view before computing the loss</li> <li>Centering is updated at every step as per DINO's original formulation</li> </ul> </li> </ul>"},{"location":"training/dino/#modular-design","title":"Modular Design","text":"Component File Role <code>DINOViT</code> <code>model.py</code> Dual backbone + head w/ EMA update <code>DINOHead</code> <code>head.py</code> Nonlinear projection head <code>DINOLoss</code> <code>loss.py</code> Self-distillation loss <code>DINOMomentumScheduler</code> <code>dino_utils.py</code> Momentum scheduler <code>DINOTrainer</code> <code>trainer.py</code> Full training loop"},{"location":"training/finetune/","title":"\ud83d\udd27 Fine-tuning Workflow","text":"<p>Fine-tuning adapts a pretrained ViT model to a new dataset. The pipeline reuses the supervised trainer but loads weights from a previous run.</p>"},{"location":"training/finetune/#overview","title":"Overview","text":"<ol> <li>Select mode \u2013 set <code>training.type: \"finetune\"</code> in the config.</li> <li>Load checkpoint \u2013 provide <code>training.pretrained_path</code> pointing to the saved model.</li> <li>Freeze layers \u2013 if <code>training.freeze_backbone: true</code>, the encoder is frozen and only the head is trained.</li> <li>Check weights \u2013 <code>load_pretrained_model()</code> matches shapes and logs any mismatches.</li> <li>Train \u2013 <code>SupervisedTrainer</code> handles the cross\u2011entropy loop and logs metrics.</li> </ol> <p>The workflow allows initializing from SimMIM, DINO or any compatible checkpoint.</p>"},{"location":"training/finetune/#typical-steps","title":"Typical Steps","text":"<pre><code># Choose the finetune config\npython train.py training.type=finetune training.pretrained_path=/path/to/checkpoint.pth\n</code></pre> <p>This can be set in the config for ease of use, as explained in the Configuration Guide</p> <p>During startup the script:</p> <ul> <li>Builds a <code>ViT</code> model with the classification head.</li> <li>Loads the pretrained weights using <code>load_pretrained_model()</code>.</li> <li>Optionally freezes backbone parameters.</li> <li>Runs <code>SupervisedTrainer.fit()</code> for the specified number of epochs.</li> </ul>"},{"location":"training/finetune/#key-components","title":"Key Components","text":"Component File Role <code>load_pretrained_model</code> <code>train.py</code> Loads weights &amp; handles shape mismatches <code>freeze_backbone</code> <code>train.py</code> Stops gradient updates for encoder blocks <code>SupervisedTrainer</code> <code>utils/train_utils.py</code> Training loop used for fine\u2011tuning <p>Fine-tuning is thus simply supervised training initialized from a checkpoint, making it easy to adapt SSL models to downstream tasks.</p>"},{"location":"training/simmim/","title":"\ud83e\udde9 SimMIM","text":"<p>This section covers the implementation of SimMIM (Simple Masked Image Modeling) in the ViT-SSL framework. It follows the original SimMIM paper.</p>"},{"location":"training/simmim/#overview","title":"Overview","text":"<p>SimMIM performs masked patch prediction, where parts of an input image are hidden (masked) and the model learns to reconstruct those regions. The approach is conceptually similar to BERT-style pretraining but adapted for vision using patch-level masking and pixel-level regression.</p>"},{"location":"training/simmim/#architecture-simmimvit","title":"Architecture: <code>SimMIMViT</code>","text":"<p>Defined in <code>model.py</code>, this model consists of:</p> <ul> <li>A custom patch embedding via <code>Unfold + Linear</code></li> <li>Learnable mask token for masked patches</li> <li>Positional embeddings</li> <li>Multiple <code>EncoderBlocks</code></li> <li>A simple MLP head for predicting RGB pixel values of masked patches</li> </ul> <p>The masked tokens are inserted directly into the sequence before encoding. No <code>[CLS]</code> token is used during pretraining.</p>"},{"location":"training/simmim/#masking-simple_masking","title":"Masking: <code>simple_masking</code>","text":"<p>Defined in <code>masking.py</code>, this function:</p> <ul> <li>Selects a random subset of patches to mask using boolean masks</li> <li>Returns:<ul> <li>The original patches (unchanged)</li> <li>A binary mask indicating which patches are masked</li> <li>The target pixels for loss computation</li> </ul> </li> </ul>"},{"location":"training/simmim/#forward-pass","title":"Forward Pass","text":"<pre><code>patches = Unfold(image)\npatches, bool_mask, targets = simple_masking(patches)\n\nencoder_input = torch.where(mask, mask_token, projected_patches)\nencoder_input += pos_emb\nencoded = transformer(encoder_input)\n\noutput = simmim_head(encoded[masked_positions])\n</code></pre> <p>The model only predicts masked patches, this makes training efficient and focused.</p>"},{"location":"training/simmim/#loss","title":"Loss","text":"<p>The loss is a pixel-wise regression loss (e.g., MSE or L1) between the predicted and ground truth pixel values of the masked patches:</p> <p><code>loss = criterion(predicted_pixels, target_pixels)</code></p>"},{"location":"training/simmim/#training-simmimtrainer","title":"Training: SimMIMTrainer","text":"<p>Implemented in trainer.py, this trainer:</p> <ul> <li>Loads input images and applies patch masking</li> <li>Flattens predictions and targets for loss computation</li> <li>Supports warmup schedulers and logs training/validation metrics</li> </ul>"},{"location":"training/simmim/#validation","title":"Validation","text":"<p>Validation follows the same logic as training, but without gradient updates. It:</p> <ul> <li>Reconstructs patches</li> <li>Measures loss</li> <li>Logs predictions for analysis or visual debugging</li> </ul> Component File Role <code>SimMIMViT</code> <code>model.py</code> Vision Transformer with patch masking and pixel prediction <code>simple_masking</code> <code>masking.py</code> Random masking of input patches and target generation <code>SimMIMTrainer</code> <code>trainer.py</code> Training/validation loop with patch reconstruction loss"},{"location":"training/supervised/","title":"\ud83e\udde0 Supervised Learning","text":"<p>This section documents the supervised classification pipeline using Vision Transformers in ViT-SSL.</p>"},{"location":"training/supervised/#architecture-vit","title":"Architecture: <code>ViT</code>","text":"<p>The model is implemented in <code>model.py</code> and follows the original ViT structure:</p> <ul> <li>Patch Embedding using a convolutional or manual unfold-based projector</li> <li>Transformer Encoder: stacked <code>EncoderBlock</code>s</li> <li>Classification Head: a LayerNorm + Linear projection (<code>MLPHead</code>) applied to the <code>[CLS]</code> token</li> </ul>"},{"location":"training/supervised/#components","title":"Components:","text":"<ul> <li><code>ConvolutionalPatchEmbedding</code>: Conv2D patch tokenizer</li> <li><code>ManualPatchEmbedding</code>: Alternative unfold + linear patching</li> <li><code>EncoderBlock</code>: Multi-head attention block</li> <li><code>MLPHead</code>: Classifier head</li> </ul>"},{"location":"training/supervised/#forward-flow","title":"Forward Flow:","text":"<pre><code>x = patch_embedding(image)\nx = encoder_blocks(x)\ncls_token = x[:, 0]\nlogits = classification_head(cls_token)\n</code></pre>"},{"location":"training/supervised/#patch-embedding-variants","title":"Patch Embedding Variants","text":"<p>Two types of patch embedding are supported:</p> Name Description <code>ConvolutionalPatchEmbedding</code> Efficient Conv2D-based tokenization <code>ManualPatchEmbedding</code> Manual Unfold + Linear patch projection <p>Each variant prepends a learnable <code>[CLS]</code> token and adds a positional embedding.</p>"},{"location":"training/supervised/#classification-head-mlphead","title":"Classification Head: MLPHead","text":"<ul> <li>A lightweight head that normalizes and linearly maps the CLS token to the number of classes.</li> <li>Can be replaced with a deeper head if needed for finetuning.</li> </ul>"},{"location":"training/supervised/#training-supervisedtrainer","title":"Training: SupervisedTrainer","text":"<p>Defined in trainer.py, this trainer handles:</p> <ul> <li>Cross-entropy loss training loop</li> <li>Accuracy tracking</li> <li>Scheduler warmup</li> <li>Backbone freezing/unfreezing for transfer learning or finetuning</li> </ul>"},{"location":"training/supervised/#finetuning-support","title":"Finetuning Support","text":"<p>If using pre-trained weights (e.g., from SimMIM or DINO), the system supports:</p> <ul> <li>Loading and matching pretrained weights with current model</li> <li>Selective layer skipping (e.g., skipping simmim_head or mask_token)</li> <li>Positional embedding interpolation if patch count differs</li> </ul> <p>For details on finetuning, see the Finetune section.</p> <p>Component Summary</p> Component File Role <code>ViT</code> <code>model.py</code> Full supervised ViT model <code>ConvolutionalPatchEmbedding</code> <code>patch_embedding.py</code> Conv2D-based patch tokenizer <code>ManualPatchEmbedding</code> <code>patch_embedding.py</code> Linear + Unfold tokenizer <code>MLPHead</code> <code>mlp_head.py</code> LayerNorm + Linear classifier <code>SupervisedTrainer</code> <code>trainer.py</code> Cross-entropy training loop with scheduler support <code>load_pretrained_model</code> <code>utils/train_utils.py</code> Weight loader with shape/positional fixups"}]}